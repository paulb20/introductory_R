---
title: "Introduction to Labour Force Survey analysis using R"
output:
  html_document: default
  html_notebook: default
---

## Load useful packages

R has many contributed packages (thousands). These ones are very useful (some more than others). It is usual to load contributed packages in a block at the beginning of the analysis sothey are then available throughout. 

I'll explain this later.

```{r setup}
library(sjPlot)
library(sjlabelled)
library(tidyverse)
library(ggeffects)
library(ggthemes)
library(sjmisc)
library(ggridges)

```

## Find the LFS data file wanted. 

In Rstudio, we could do this with a dialog box (File > Import Dataset), but that's just one way of doing it. 

In R, there are almost always many ways of doing the same thing, some are easier to learn, some are quicker to do.  What I'm trying to do here is to encourage what is called 'reproducible research', so that the code can be run with the same data and always produce the same results, and new data can be substituted and an updated version produced without further tweaking.

```{r}
shortnames <- list.files("../Quarterly_Q1name/", pattern= "^.*\\.sav$")
lfsdir <- "../Quarterly_Q1name/"
tail(shortnames)
to_load <- paste0(lfsdir,tail(shortnames,1))

```

What we did there was:

* Get a listing of the SPSS *.sav files in the directory and put it into an R object known as a vector.
* The *list.files()* command would give a list of all the files in the directory. We restricted it to .sav files by giving it a pattern to search for. Pattern matching is a subject for another day...
* We set the location of the files into a second named R object.
* We used the *tail()* command to simply list the last five names in the shortnames object.
* Then we've pasted the lfs directory and the last one in the list of files together into a 'to_load' object.

## Load the LFS data file

```{r, message=FALSE}
latestlfs <- read_spss(to_load)
```

This loads the entire latest quarterly LFs data file into the computer's memory. The survey has `r nrow(latestlfs)` observations on `r ncol(latestlfs)` variables.

Note: this code is general. Adding an extra data file to the list will mean a different file is loaded.

Therefore, simply looking at the data file is not particularly easy.

So here are a few interesting simple tables.


```{r}
sjt.frq(latestlfs$MF1664, title="Whether aged 16-64 (survey includes children, pensioners)")
sjt.frq(latestlfs$ILODEFR, title="Employment Status")
#sjt.frq(latestlfs$LEVQUL15, title="Qualification level (not asked of under 16 or over 70)")

```

These tables are produced by the sjt.frq() command. This is part of a set of contributed packages that work well for social science research, where much of the data used is in categorical form.


### Economic activity by qualification, within age-group

```{r}
latestlfs %>% filter(MF1664 %in% 1:2)  %>% group_by(MF1664) %>% select(LEVQUL15, ILODEFR, MF1664) %>% sjtab(fun="xtab", show.row.prc=TRUE)
```

### Weighted and unweighted results

These results are not weighted to population totals. This has been done deliberately to show how many actual survey respondents we have in the different categories. The survey includes weights (different ones for the overall survey and for the 40% asked about earnings). 

It would be useful to use the weights and the survey design information that is encoded in the weights to give us confidence intervals for the estimates. We do find that response rates are not entirely random, and therefore the weights vary across the survey. In particular, populations of interest (such as NEET young people) may have low response rates, or be disproportionately likely to be 'proxy interviews' by another household member. Therefore, being aware of these issues is important.

This chunk loads the survey packages that handle weighted variables. It then recodes qualifications so that qualification levels are 'the right way up' i.e. NQF Level 4 and over is 4, NQF level 2 is 2, so that we can work out average qualification levels for particular groups. Thirdly, create variables that are categorical copies of several key variables, and we then convert thee dataset into weighted form.

The R software recognises upper and lower case letters as different, so it is possible to use a convention as I have done here, so that 

```{r}
library(survey)
library(srvyr)
latestlfs$levqualnum <- rec(latestlfs$LEVQUL15, rec="1=4; 2=3; 3=2.5; 4=2; 5=1; 6=NA; 7=0", val.labels=c("No qualifications", "Below NQF Level 2", "NQF Level 2","Trade apprenticeships", "NQF Level 3",  "NQF Level 4 and above"))
latestlfs$levqul15 <- as_label(latestlfs$LEVQUL15, drop.levels = TRUE)
latestlfs$sc10mmj <- as_label(latestlfs$SC10MMJ, drop.levels = TRUE)
latestlfs$sc10mmn <- as_label(latestlfs$SC10MMN, drop.levels = TRUE)
latestlfs_srvyr <- as_survey(latestlfs, 1, weights = PWT17)

```

### Charting qualifications valued by employers

We'll build up a chart bit by bit. First we need to summarise the data so that the chart can be built.

```{r}
latestlfs_srvyr %>% 
  group_by(sc10mmj) %>% 
  summarize(mean_qual = survey_mean(levqualnum, vartype="ci", na.rm=TRUE), n = survey_total(na.rm=TRUE))
```
#### The number of workers by qualification level

We take our weighted survey file, then 'pipe' it (%>%) through a grouping variable - in this case, occupation of workers (at top level grouping) and then summarise the average qualification level. We tell the program to identify a confidence interval for the average, which is based on the number of survey respondents and the rates at which people respond to the survey.  This gives us lower and upper confidence intervals for the average qualification level. We are 95% sure that the true average is within the confidence interval. We also tell the program to calculate the weighted number of workers in each group. Because this number is also uncertain, the program calculates a quality measure for that number as well.

#### The number of recruits by qualification level

We add one extra element in the pipe to filter the data so that we are only looking at people who started a new job in the last three months.

```{r}
latestlfs_srvyr %>% 
  filter(EMPLEN ==1) %>%
  group_by(sc10mmj) %>% 
  summarize(mean_qual = survey_mean(levqualnum, vartype="ci", na.rm=TRUE), n = survey_total(na.rm=TRUE))
```

This produces a table that is identically structured, but has many fewer people counted as recruits.

We could have saved each table as an R object in the workspace, but we haven't done that so we haven't cluttered up the workspace.

Now we get to produce a chart

```{r}
latestlfs_srvyr %>% 
  filter(EMPLEN==1) %>% 
  group_by(sc10mmj) %>% 
  summarize(mean_qual = survey_mean(levqualnum, vartype="ci", na.rm=TRUE), n = survey_total(na.rm=TRUE)) %>% 
  ggplot(aes(sc10mmj, mean_qual)) + 
  geom_pointrange(aes(ymin=mean_qual_low, ymax=mean_qual_upp, size=n)) + 
  coord_flip() + 
  scale_size(range=c(0,1), guide=guide_legend(title="Number of recruits")) + 
  labs(title="Average qualification level of recruits by occupation", x= "Occupation group", y="Average NQF qualification level")
```

Here, we've started from the weighted survey, created the table we had before, then piped it into 'ggplot' which creates a chart in layers.

Firstly we've identified the main x and y variables, then we've told it what sort of chart to plot - a 'pointrange', which needs two additional variables - the lower and upper confidence intervals. We've also added the number of recruits in this layer. We could have added it in the original layer, which might have meant that the line didn't increase in size with the dot. I prefer the way I've done it here.I've then flipped the chart on its side, which means that the labels for the occupation groups don't overwrite each other, specified a size range for the dots (the standard was much bigger) and added titles.

Further stages with the chart (not done here so we can show a simpler version) are to 'theme' the chart in L&W standard fonts, colours and lines. The ones we use in the monthly labour market stats briefing are based on the standard used by the Economist magazine, but using our colours and fonts.

### Discussion

This analysis shows something about the qualifications employers are recruiting. We can repeat the analysis with a more detailed level of occupation coding with just one change (the variable name), or changing the qualification coding (rather lengthier) or use different variables such as the age someone achieved their highest qualification.

The confidence intervals around the average recruitment qualification from using just one Labour Force Survey dataset, for the UK are reasonable. The main overlaps are in the mid-skills area. If we needed to do the same analysis for a smaller area, (e.g. Wales) the confidence intervals would be wider. The way to handle that would be to 'pool' datasets so that we had numbers of actual survey respondents which were closer to the ones we have on a UK basis - so 8 or 12 datasets which would give a total number of recruits over two or three years. 

Let's try a Wales version with just one dataset.I've changed the title and added an extra filter, but the code is otherwise identical.


```{r}
latestlfs_srvyr %>% 
  filter(EMPLEN==1, GOVTOF2 == 11) %>% 
  group_by(sc10mmj) %>% 
  summarize(mean_qual = survey_mean(levqualnum, vartype="ci", na.rm=TRUE), n = survey_total(na.rm=TRUE)) %>% 
  ggplot(aes(sc10mmj, mean_qual)) + 
  geom_pointrange(aes(ymin=mean_qual_low, ymax=mean_qual_upp, size=n)) + 
  coord_flip() + 
  scale_size(range=c(0,1), guide=guide_legend(title="Number of recruits")) + 
  labs(title="Wales average qualification level of recruits by occupation", x= "Occupation group", y="Average NQF qualification level")
```

The Office for National Statistics recommend that people should not use estimates under 10,000. Looking at those confidence intervals shows the reason they make that sort of recommendation. None of the individual categories has more than 10,000 recruits in one quarter, and the cinfidence intervals are such that we can't even be sure that professional and associate professional recruits are better qualified than plant and machine operatives.

The number of recruits in Wales in the survey into managerial positions is so small that a confidence interval cannot be calculated. It is actually based on 2 respondents. The biggest dots here are based on 10 respondents. With those numbers, if 12 datasets had identical patterns, there would be 24 managerial recruits, and 120 in elementary occupations or caring, leisure etc. That's still much smaller than the UK single quarter dataset, but may be workable with. 

There are trade-offs here between the timeliness of the analysis and the robustness of it. We could add all the datasets with the same occupation coding (which is now about 6 years - 24 datasets). This would give a much better classification, but the labour market was different in 2013, and higher qualified people were being recruited further down the occupation ladder.

### Averages are not the only way to look at patterns.

In fact, the distribution around the average can be very important. The confidence interval for the average does reflect some of the distibution, but not all of it.

Let's look at the distribution of qualifications for job starters within each of these 9 occupation groups. 

To do this we'll use a method known as 'small multiples', with multiple charts in one picture. These aren't scaled by the number of recruits, and are unweighted, but you'll get the picture. We're using the long list of qualifications which runs from 1 which is higher degrees to 84 which is no qualifications. We're assuming that the Office for National Statistics has got the ordering of qualifications broadly right within that.

```{r}
latestlfs %>% filter(EMPLEN == 1, !is.na(SC10MMJ)) %>% ggplot(aes(x=HIQUAL15, y=to_label(SC10MMJ))) + geom_density_ridges() + labs(title="Qualifications of new job starters, AJ 2017", x="Qualification, 1=highest qualified, 84=no qualifications", y="Occupation") + theme_ridges()

```

This shows that each of the occupation groups has a wide variety of recruits, with the most concentrated distribution for professional occupations. However, even for them there are numbers of recruits at lower levels . For example, accountants 'qualified by experience' have always been regarded as part of the profession, but may find moving jobs being more difficult than progressiong within a firm.

The only occupation group where the lowest qualified have a large share of recruits is 'process, plat & machine operatives', whereas elementary occupations has a peak at around Level 2 - probably reflecting both the share of the population and the extent to which those jobs are done by young people while studying for higher level qualifications.Notably, the 'process plant recruits, while they have a peak at the bottom end, have substantial numbers recruited higher up. This pulls the average recruitment level up to the level we saw earlier.

The various peaks in the distribution relate to the qualification levels.

The individual plots within the chart are plotting not actual numbers, but probability densities. These are smoothed curves, so go slightly wider than the actual numbers in the data.

